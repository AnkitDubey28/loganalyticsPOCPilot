"""
Cipher Agent - Insights, anomaly detection, and recommendations
"""
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from collections import Counter
import urllib.request
import json
import re

class Cipher:
    def __init__(self, ledger):
        self.ledger = ledger
        self.cloud_patterns = {
            'aws': [
                r'aws', r'amazon', r's3\.', r'ec2', r'lambda', r'cloudwatch',
                r'cloudtrail', r'elasticache', r'dynamodb', r'rds', r'elb',
                r'eventName', r'requestParameters', r'sourceIPAddress',
                r'arn:aws:', r'amazonaws\.com'
            ],
            'azure': [
                r'azure', r'microsoft', r'resourceId', r'\.windows\.net',
                r'subscriptionId', r'resourceGroupName', r'operationName',
                r'activityLog', r'properties\.', r'correlation',
                r'az\.', r'\.azurewebsites\.'
            ],
            'gcp': [
                r'gcp', r'google', r'googleapis', r'compute\.', r'storage\.', 
                r'cloud\.google', r'gke', r'bigquery', r'pubsub',
                r'protoPayload', r'insertId', r'logName', r'projects/'
            ]
        }
    
    def _detect_cloud_provider(self, log_content):
        """Detect cloud provider from log content using pattern matching"""
        if not log_content:
            return 'other'
        
        log_str = str(log_content).lower()
        scores = {'aws': 0, 'azure': 0, 'gcp': 0}
        
        for provider, patterns in self.cloud_patterns.items():
            for pattern in patterns:
                matches = len(re.findall(pattern, log_str, re.IGNORECASE))
                scores[provider] += matches
        
        # Return provider with highest score, or 'other' if no matches
        if max(scores.values()) > 0:
            return max(scores, key=scores.get)
        return 'other'
    
    def compute_insights(self, time_window_hours=24):
        """Compute comprehensive insights"""
        # Get recent events
        events = self.ledger.list_events(limit=10000)
        
        if not events:
            return {'success': False, 'reason': 'No events found'}
        
        df = pd.DataFrame(events)
        
        # Ensure timestamp parsing
        if 'ts_event' in df.columns:
            try:
                df['ts_event'] = pd.to_datetime(df['ts_event'], errors='coerce')
                df = df.dropna(subset=['ts_event'])
            except:
                pass
        
        insights = {
            'success': True,
            'generated_at': datetime.utcnow().isoformat(),
            'time_window_hours': time_window_hours
        }
        
        # Error analysis
        errors = df[df['level'].isin(['ERROR', 'FATAL', 'CRITICAL', 'error', 'fatal', 'critical'])]
        insights['error_count'] = len(errors)
        insights['error_rate'] = round(len(errors) / len(df) * 100, 2) if len(df) > 0 else 0
        
        # Spike detection using MAD (Median Absolute Deviation)
        if 'ts_event' in df.columns and len(df) > 50:
            df_sorted = df.sort_values('ts_event')
            df_sorted['hour'] = df_sorted['ts_event'].dt.floor('H')
            hourly_counts = df_sorted.groupby('hour').size()
            
            median = hourly_counts.median()
            mad = np.median(np.abs(hourly_counts - median))
            threshold = median + 3 * mad
            
            spikes = hourly_counts[hourly_counts > threshold]
            insights['spikes'] = [
                {'timestamp': str(ts), 'count': int(count)}
                for ts, count in spikes.items()
            ]
        else:
            insights['spikes'] = []
        
        # Top offenders
        if 'service' in df.columns:
            insights['top_services'] = [
                {'name': k, 'count': v}
                for k, v in Counter(df['service'].dropna()).most_common(10)
            ]
        else:
            insights['top_services'] = []
        
        if 'user_identity' in df.columns:
            insights['top_users'] = [
                {'name': k, 'count': v}
                for k, v in Counter(df['user_identity'].dropna()).most_common(10)
            ]
        else:
            insights['top_users'] = []
        
        if 'ip_address' in df.columns:
            insights['top_ips'] = [
                {'name': k, 'count': v}
                for k, v in Counter(df['ip_address'].dropna()).most_common(10)
            ]
        else:
            insights['top_ips'] = []
        
        # Error message patterns
        if len(errors) > 0 and 'message' in errors.columns:
            error_keywords = []
            for msg in errors['message'].dropna():
                words = str(msg).lower().split()
                error_keywords.extend([w for w in words if len(w) > 4])
            
            insights['top_error_keywords'] = [
                {'keyword': k, 'count': v}
                for k, v in Counter(error_keywords).most_common(15)
            ]
        else:
            insights['top_error_keywords'] = []
        
        # Recommendations
        insights['recommendations'] = self._generate_recommendations(insights)
        
        # Compliance checks
        insights['compliance'] = self._check_compliance(df)
        
        # Cloud comparison
        insights['cloud_comparison'] = self._cloud_comparison()
        
        # Knowledge articles
        insights['knowledge_articles'] = self._get_knowledge_articles(insights)
        
        return insights
    
    def _get_knowledge_articles(self, insights):
        """Generate relevant knowledge articles based on insights"""
        articles = []
        
        if insights.get('error_rate', 0) > 10:
            articles.append({
                'title': 'Best Practices for Error Handling',
                'category': 'Error Management',
                'summary': 'Learn how to implement robust error handling patterns to reduce error rates and improve system reliability.',
                'tags': ['errors', 'reliability', 'best-practices'],
                'read_time': '5 min',
                'icon': 'ðŸ“š'
            })
        
        if len(insights.get('spikes', [])) > 0:
            articles.append({
                'title': 'Auto-Scaling Strategies for Variable Workloads',
                'category': 'Performance',
                'summary': 'Implement intelligent auto-scaling to handle traffic spikes efficiently while controlling costs.',
                'tags': ['scaling', 'performance', 'cost-optimization'],
                'read_time': '7 min',
                'icon': 'ðŸ“ˆ'
            })
        
        articles.append({
            'title': 'Cloud Logging Best Practices: AWS vs Azure vs GCP',
            'category': 'Cloud Comparison',
            'summary': 'Compare logging capabilities, costs, and features across major cloud providers to optimize your logging strategy.',
            'tags': ['aws', 'azure', 'gcp', 'comparison'],
            'read_time': '10 min',
            'icon': 'â˜ï¸'
        })
        
        articles.append({
            'title': 'Cost Optimization: Reduce Cloud Logging Expenses by 40%',
            'category': 'Cost Optimization',
            'summary': 'Proven techniques to reduce logging costs including sampling, filtering, and intelligent retention policies.',
            'tags': ['cost', 'optimization', 'savings'],
            'read_time': '8 min',
            'icon': 'ðŸ’°'
        })
        
        articles.append({
            'title': 'Security Monitoring with Log Analytics',
            'category': 'Security',
            'summary': 'Detect security threats early by implementing effective log monitoring and alerting strategies.',
            'tags': ['security', 'monitoring', 'alerts'],
            'read_time': '6 min',
            'icon': 'ðŸ”’'
        })
        
        articles.append({
            'title': 'Database Performance Tuning Guide',
            'category': 'Performance',
            'summary': 'Optimize database queries and connections to reduce latency and improve application performance.',
            'tags': ['database', 'performance', 'optimization'],
            'read_time': '9 min',
            'icon': 'âš¡'
        })
        
        return articles
    
    def _generate_recommendations(self, insights):
        """Generate actionable recommendations with cost optimization"""
        recs = []
        
        # Critical Error Rate
        if insights.get('error_rate', 0) > 10:
            recs.append({
                'priority': 'CRITICAL',
                'category': 'Error Rate',
                'title': 'High Error Rate Detected',
                'message': f"Error rate is {insights['error_rate']}% (threshold: 10%). This indicates system instability.",
                'action': 'Review top error services and messages',
                'impact': 'High',
                'cost_impact': 'Errors may cause retries, increasing compute costs by 15-30%',
                'icon': 'ðŸš¨'
            })
        
        # Traffic Anomalies
        if len(insights.get('spikes', [])) > 0:
            recs.append({
                'priority': 'HIGH',
                'category': 'Traffic Anomaly',
                'title': 'Traffic Spikes Detected',
                'message': f"Detected {len(insights['spikes'])} traffic spikes. Could indicate DDoS or batch processing issues.",
                'action': 'Review spike patterns and implement auto-scaling',
                'impact': 'Medium',
                'cost_impact': 'Spikes may trigger unnecessary scaling. Implement predictive scaling to save 20% on compute.',
                'icon': 'ðŸ“ˆ'
            })
        
        # Service Health
        if len(insights.get('top_services', [])) > 0:
            top_service = insights['top_services'][0]
            if top_service['count'] > insights.get('error_count', 0) * 0.3:
                recs.append({
                    'priority': 'MEDIUM',
                    'category': 'Service Health',
                    'title': 'Service Optimization Needed',
                    'message': f"Service '{top_service['name']}' has high activity ({top_service['count']} events).",
                    'action': 'Monitor resource utilization and implement caching',
                    'impact': 'Medium',
                    'cost_impact': 'Consider caching frequently accessed data to reduce database calls by 40%',
                    'icon': 'âš™ï¸'
                })
        
        # Cost Optimization - Log Volume
        total_events = insights.get('error_count', 0)
        if total_events > 5000:
            recs.append({
                'priority': 'LOW',
                'category': 'Cost Optimization',
                'title': 'Reduce Log Volume',
                'message': f'Processing {total_events} events. High log volume increases storage and processing costs.',
                'action': 'Implement log sampling, filtering, or aggregation',
                'impact': 'Low',
                'cost_impact': 'Log reduction can save 30-50% on storage and indexing costs',
                'icon': 'ðŸ’°'
            })
        
        # Cost Optimization - Idle Resources
        if len(insights.get('top_users', [])) > 0:
            user_count = len(insights.get('top_users', []))
            if user_count < 5:
                recs.append({
                    'priority': 'LOW',
                    'category': 'Cost Optimization',
                    'title': 'Low Usage Detected',
                    'message': f'Only {user_count} active users detected. Resources may be over-provisioned.',
                    'action': 'Right-size infrastructure based on actual usage patterns',
                    'impact': 'Low',
                    'cost_impact': 'Right-sizing can reduce infrastructure costs by 25-40%',
                    'icon': 'ðŸ’¡'
                })
        
        # Security Recommendation
        if insights.get('error_rate', 0) > 5:
            recs.append({
                'priority': 'MEDIUM',
                'category': 'Security',
                'title': 'Implement Proactive Monitoring',
                'message': 'Elevated error rates may indicate security issues or misconfigurations.',
                'action': 'Enable real-time alerting and implement WAF rules',
                'impact': 'Medium',
                'cost_impact': 'Proactive monitoring prevents downtime, saving potential revenue loss',
                'icon': 'ðŸ”’'
            })
        
        # Performance Optimization
        recs.append({
            'priority': 'LOW',
            'category': 'Performance',
            'title': 'Database Query Optimization',
            'message': 'Optimize frequent database queries to reduce latency.',
            'action': 'Implement connection pooling and query caching',
            'impact': 'Low',
            'cost_impact': 'Query optimization can reduce database costs by 15-25%',
            'icon': 'âš¡'
        })
        
        if not recs:
            recs.append({
                'priority': 'INFO',
                'category': 'Health',
                'title': 'System Healthy',
                'message': 'No critical issues detected. System is operating normally.',
                'action': 'Continue monitoring and maintain current practices',
                'impact': 'None',
                'cost_impact': 'Maintain current cost levels',
                'icon': 'âœ…'
            })
        
        return recs
    
    def _check_compliance(self, df):
        """Check basic compliance indicators"""
        checks = []
        
        # Logging coverage
        checks.append({
            'name': 'Logging Coverage',
            'status': 'PASS' if len(df) > 100 else 'WARNING',
            'message': f'Total events: {len(df)}'
        })
        
        # Timestamp presence
        ts_coverage = df['ts_event'].notna().sum() / len(df) * 100 if len(df) > 0 else 0
        checks.append({
            'name': 'Timestamp Coverage',
            'status': 'PASS' if ts_coverage > 90 else 'FAIL',
            'message': f'{ts_coverage:.1f}% events have timestamps'
        })
        
        # User tracking
        user_coverage = df['user_identity'].notna().sum() / len(df) * 100 if len(df) > 0 else 0
        checks.append({
            'name': 'User Identity Tracking',
            'status': 'PASS' if user_coverage > 50 else 'WARNING',
            'message': f'{user_coverage:.1f}% events have user identity'
        })
        
        return checks
    
    def _fetch_realtime_cloud_data(self):
        """Fetch real-time cloud pricing and recommendations from internet"""
        realtime_data = {}
        
        # Try to fetch real-time AWS pricing (simplified example)
        try:
            # This is a simplified approach - in production, you'd use official APIs
            # For demonstration, we'll simulate fetching from a pricing API
            url = "https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/index.json"
            req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
            
            with urllib.request.urlopen(req, timeout=5) as response:
                data = json.loads(response.read().decode())
                realtime_data['aws_available'] = True
        except:
            # Fallback to static data if internet unavailable
            realtime_data['aws_available'] = False
        
        # Fallback pricing data (updated Dec 2025)
        realtime_data['pricing'] = {
            'aws': {
                'cloudwatch_logs': '$0.50 per GB ingested',
                's3_standard': '$0.023 per GB/month',
                'recommendations': [
                    'Enable S3 Intelligent-Tiering for automatic cost savings',
                    'Use CloudWatch Logs Insights for efficient querying',
                    'Set up log retention policies to reduce storage costs',
                    'Consider CloudWatch Contributor Insights for pattern analysis'
                ]
            },
            'azure': {
                'log_analytics': '$2.30 per GB ingested',
                'storage': '$0.025 per GB/month',
                'recommendations': [
                    'Use Azure Monitor commitment tiers (up to 50% savings)',
                    'Enable Log Analytics workspace data retention policies',
                    'Consider Azure Data Explorer for complex analytics',
                    'Use diagnostic settings to control log granularity'
                ]
            },
            'gcp': {
                'cloud_logging': '$0.50 per GB ingested',
                'storage': '$0.020 per GB/month',
                'recommendations': [
                    'Use log exclusion filters to reduce ingestion costs',
                    'Enable log sampling for high-volume services',
                    'Leverage BigQuery for cost-effective long-term storage',
                    'Set up log sinks to route logs efficiently'
                ]
            }
        }
        
        return realtime_data
    
    def _cloud_comparison(self):
        """Compare characteristics across cloud providers with insights"""
        files = self.ledger.list_files()
        events = self.ledger.list_events(limit=1000)
        
        # Fetch real-time data from internet
        realtime_data = self._fetch_realtime_cloud_data()
        
        # Detect cloud providers from actual log content
        detected_clouds = {}
        for event in events[:500]:  # Sample first 500 events for detection
            # Create a string with all relevant fields for detection
            log_content = ' '.join([
                str(event.get('message', '')),
                str(event.get('service', '')),
                str(event.get('raw_log', ''))
            ])
            
            provider = self._detect_cloud_provider(log_content)
            detected_clouds[provider] = detected_clouds.get(provider, 0) + 1
        
        # Get real-time pricing from internet
        pricing_data = realtime_data.get('pricing', {})
        
        cloud_stats = {
            'aws': {
                'count': 0, 
                'size': 0, 
                'detected': detected_clouds.get('aws', 0),
                'name': 'AWS (Amazon Web Services)',
                'icon': 'â˜ï¸',
                'color': '#FF9900',
                'strengths': ['Industry leader with 200+ services', 'CloudTrail for comprehensive audit logs', 'Mature monitoring with CloudWatch', 'Best for: Enterprises, startups, general workloads'],
                'cost_tip': pricing_data.get('aws', {}).get('recommendations', ['Use S3 Intelligent-Tiering'])[0] if pricing_data.get('aws') else 'Use S3 Intelligent-Tiering for log storage',
                'avg_cost_per_gb': '$0.023',
                'ingestion_cost': pricing_data.get('aws', {}).get('cloudwatch_logs', '$0.50 per GB'),
                'storage_cost': pricing_data.get('aws', {}).get('s3_standard', '$0.023 per GB/month'),
                'recommendations': pricing_data.get('aws', {}).get('recommendations', [])
            },
            'azure': {
                'count': 0, 
                'size': 0,
                'detected': detected_clouds.get('azure', 0),
                'name': 'Microsoft Azure',
                'icon': 'âš¡',
                'color': '#0078D4',
                'strengths': ['Deep Microsoft ecosystem integration', 'Log Analytics workspace for centralized logging', 'Strong hybrid cloud support', 'Best for: Enterprises, .NET applications, hybrid scenarios'],
                'cost_tip': pricing_data.get('azure', {}).get('recommendations', ['Use commitment tiers'])[0] if pricing_data.get('azure') else 'Use commitment tiers for predictable savings (up to 50% off)',
                'avg_cost_per_gb': '$0.025',
                'ingestion_cost': pricing_data.get('azure', {}).get('log_analytics', '$2.30 per GB'),
                'storage_cost': pricing_data.get('azure', {}).get('storage', '$0.025 per GB/month'),
                'recommendations': pricing_data.get('azure', {}).get('recommendations', [])
            },
            'gcp': {
                'count': 0, 
                'size': 0,
                'detected': detected_clouds.get('gcp', 0),
                'name': 'Google Cloud Platform',
                'icon': 'ðŸ”·',
                'color': '#4285F4',
                'strengths': ['Advanced data analytics with BigQuery', 'Best-in-class Kubernetes (GKE)', 'Strong ML/AI integration', 'Best for: Data-intensive apps, containerized workloads'],
                'cost_tip': pricing_data.get('gcp', {}).get('recommendations', ['Use log exclusions'])[0] if pricing_data.get('gcp') else 'Use log exclusion filters and sampling to reduce costs by 40-60%',
                'avg_cost_per_gb': '$0.020',
                'ingestion_cost': pricing_data.get('gcp', {}).get('cloud_logging', '$0.50 per GB'),
                'storage_cost': pricing_data.get('gcp', {}).get('storage', '$0.020 per GB/month'),
                'recommendations': pricing_data.get('gcp', {}).get('recommendations', [])
            },
            'other': {
                'count': 0, 
                'size': 0,
                'detected': detected_clouds.get('other', 0),
                'name': 'Other/On-Premise/Multi-Cloud',
                'icon': 'ðŸ¢',
                'color': '#6c757d',
                'strengths': ['Full control over infrastructure', 'No data egress fees', 'Custom compliance controls', 'Best for: Regulated industries, legacy systems'],
                'cost_tip': 'Consider managed cloud services for better ROI and reduced maintenance',
                'avg_cost_per_gb': '$0.015',
                'ingestion_cost': 'Variable (infrastructure costs)',
                'storage_cost': '$0.015 per GB/month (estimated)',
                'recommendations': ['Evaluate migration to managed cloud services', 'Implement centralized logging platform', 'Use open-source tools like ELK or Grafana Loki']
            }
        }
        
        # Count files by detected cloud type
        for f in files:
            # Try to detect cloud type from filename or stored metadata
            cloud = f.get('cloud_type', 'other')
            if cloud not in cloud_stats:
                cloud = 'other'
            cloud_stats[cloud]['count'] += 1
            cloud_stats[cloud]['size'] += f.get('size', 0)
        
        # Always show all cloud providers for comparison, not just detected ones
        comparison = []
        for k, v in cloud_stats.items():
            size_gb = v['size'] / (1024*1024*1024) if v['size'] > 0 else 0
            
            # Determine if this cloud is actively used
            is_active = v['detected'] > 0 or v['count'] > 0
            
            comparison.append({
                'cloud': v['name'],
                'icon': v['icon'],
                'color': v['color'],
                'files': v['count'],
                'detected_events': v['detected'],
                'is_active': is_active,
                'total_size_mb': round(v['size'] / (1024*1024), 2),
                'total_size_gb': round(size_gb, 4),
                'estimated_monthly_cost': f"${round(size_gb * float(v['avg_cost_per_gb'].replace('$', '')), 2)}" if size_gb > 0 else "$0.00",
                'ingestion_cost': v.get('ingestion_cost', 'N/A'),
                'storage_cost': v.get('storage_cost', 'N/A'),
                'strengths': v['strengths'],
                'cost_tip': v['cost_tip'],
                'avg_cost_per_gb': v['avg_cost_per_gb'],
                'recommendations': v.get('recommendations', [])
            })
        
        # Sort by activity (detected events) to show active provider first
        comparison.sort(key=lambda x: x['detected_events'], reverse=True)
        
        return comparison
